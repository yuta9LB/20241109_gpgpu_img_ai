{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 機械学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/machine_learning.png\" width=\"800\"></img>  \n",
    "引用元: [AINOW[初心者でもわかるディープラーニングー基礎知識からAIとの違い、導入プロセスまで細かく解説]](https://ainow.ai/2019/08/06/174245/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI**（Artificial Intelligence、人工知能） = 人間の知的な行動を模倣するコンピュータシステム全般を指す**広い概念**  \n",
    "人間のように**思考し、学習し、判断し、解決できるシステム**（例えば選択式のチャットボットとか）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 機械学習とは\n",
    "- コンピュータが**データ**から学び、新しい情報や問題に対応する技術や方法\n",
    "- **データ**から自動的にパターンを「**学習**」し、予測や意思決定を行うことができる\n",
    "\n",
    "例）お掃除ロボット、チャットボット、店舗来客分析、生産量予測などなど\n",
    "\n",
    "<img src='img/chatbot_ai.png'></img>  \n",
    "引用元: [qualva[チャットボットとは？種類・仕組みごとの特徴、導入する目的を徹底解説！]](https://qualva.com/qualvatics/archive/477/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主な種類\n",
    "- <b>教師あり学習</b>：入力データに対する正解データを用いてモデルを訓練し、新しいデータに予測を行う\n",
    "- 教師無し学習：データ内の傾向やパターンを見つけ出す\n",
    "- 強化学習：エージェント（AI）に試行錯誤を行わせながら、最適な挙動をするよう学習させる\n",
    "\n",
    "今回行うのは教師あり学習！！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深層学習（ディープラーニング）とは\n",
    "<img src=\"img/neural_net.png\" width=\"800\"></img>  \n",
    "引用元: [翔泳社[ディープラーニングと脳の関係とは？ 人工ニューロンや再帰型ニューラルネットワークを解説]](https://www.shoeisha.co.jp/book/article/detail/304)\n",
    "\n",
    "- 多層の「**ニューラルネットワーク**」を使い、**複雑な特徴**を学習する手法\n",
    "- 従来の手法では困難だった高度なタスク（画像認識、音声認識、自然言語処理など）を高精度で実現  \n",
    "\n",
    "例）畳み込みニューラルネットワーク（CNN）、リカレントニューラルネットワーク（RNN）など"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 機械学習の主な手順\n",
    "1. <b>データの収集・整理</b>\n",
    "    - 数値、テキスト、画像、音声など、タスクに応じて\n",
    "    - 十分な質と量が必要で、一般的にデータが多いほど精度を向上させられる\n",
    "\n",
    "2. <b>前処理</b>\n",
    "    - モデルが学習しやすいように、データを変換したり、特徴量を抽出したり\n",
    "\n",
    "3. <b>モデル・評価基準の設定</b>\n",
    "    - モデル＝未知のデータセットからパターンを発見したり、予測のためのルールを構築するプログラム\n",
    "    - 評価基準＝モデルがどの程度の精度で予測できているかを定量的に計測するもの（損失関数・評価関数）\n",
    "\n",
    "4. <b>学習</b>\n",
    "    - 問題に適したアルゴリズムで、訓練用データを使ってモデルを訓練\n",
    "    - 評価基準をもとに、より良い精度を実現しようと学習する\n",
    "\n",
    "5. <b>結果の可視化</b>\n",
    "    - 評価指標を算出し、グラフにプロット\n",
    "    - 予測結果を出力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本日のタスク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セマンティックセグメンテーション（Semantic Segmentation）\n",
    "- 画像内のピクセルごとに「何が映っているか」を識別し、それぞれを特定のカテゴリに分類する技術\n",
    "- 医療現場や自動運転技術など、様々な場所で使われている  \n",
    "\n",
    "<img src=\"img/semantic_segmentation.png\" width=\"800\"></img>  \n",
    "引用元: [DAGS[Scene Understanding Datasets]](http://dags.stanford.edu/projects/scenedataset.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用するモデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNet\n",
    "- 構造\n",
    "    - U字型のネットワーク構造を持つ、エンコーダとデコーダの組み合わせ\n",
    "    - エンコーダ（下り）は画像の特徴を抽出、デコーダ（上り）はそれを使って元の解像度での予測を行う\n",
    "- 特徴\n",
    "    - スキップ接続：エンコーダの各層からデコーダに情報を直接渡す「スキップ接続」を持つため、低レベルの特徴が失われにくく、詳細な予測が可能\n",
    "    - 主な用途：医療画像解析でのセマンティックセグメンテーションに多用され、特に腫瘍や臓器などの境界が複雑な領域を識別するのに適している\n",
    "\n",
    "<img src=\"img/unet.png\" width=\"800\"></img><br>\n",
    "引用元: [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSPNet（Pyramid Scene Parsing Network）\n",
    "- 構造\n",
    "    - ピラミッドプーリングモジュール（PPM）という特殊な構造で、画像全体を異なるスケールでプールして統合する仕組み\n",
    "- 特徴\n",
    "    - ピラミッドプーリングモジュール: 画像を異なるスケール（大きさ）で処理し、広範な周辺情報を捉えることで、シーン全体の理解を高める\n",
    "    - 遠く離れた要素同士の関連も考慮可能に\n",
    "    - 主な用途：シーン解析や自動運転、都市環境の画像解析など、広範囲な構成が重要となる場面で活用\n",
    "\n",
    "<img src=\"img/pspnet.png\" width=\"800\"></img><br>\n",
    "引用元： [Pyramid Scene Parsing Network]('https://arxiv.org/abs/1612.01105')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実際に学習を体験"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先ほどの手順に沿って行ってみます。\n",
    "><b>機械学習の主な手順</b>\n",
    ">1. データの収集・整理\n",
    ">2. 前処理\n",
    ">3. モデル・評価基準の設定\n",
    ">4. 学習\n",
    ">5. 結果の可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリのインポート\n",
    "ライブラリ：Pythonで開発を行うにあたって良く使われる関数やパッケージがまとめられたもの（必要な道具）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from unet import UNet\n",
    "from dataset import VOCDataset\n",
    "from util import DiceCrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. データの収集・整理 + 2. 前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 今回使用するデータ\n",
    "[Visual Object Classes Challenge 2012 (VOC2012)](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html#devkit)  \n",
    "- 検出対象は20種類で、「背景」と「未分類」合わせると22クラス\n",
    "- 訓練データが1464枚、テストデータが1449枚\n",
    "\n",
    ">@misc{pascal-voc-2012,\n",
    ">\tauthor = \"Everingham, M. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.\",\n",
    ">\ttitle = \"The {PASCAL} {V}isual {O}bject {C}lasses {C}hallenge 2012 {(VOC2012)} {R}esults\",\n",
    ">\thowpublished = \"http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像サンプル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力画像・正解画像の例\n",
    "input = Image.open('VOCdevkit/VOC2012_sample/JPEGImages/2007_000032.jpg')\n",
    "gt = Image.open('VOCdevkit/VOC2012_sample/SegmentationClass/2007_000032.png')\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "fig.add_subplot(2,3,1).set_title('input')\n",
    "plt.imshow(input)\n",
    "fig.add_subplot(2,3,2).set_title('gt')\n",
    "plt.imshow(gt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### カラーパレット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC2012で用いるラベル\n",
    "CLASSES = ['backgrounds','aeroplane','bicycle','bird','boat','bottle',\n",
    "            'bus','car' ,'cat','chair','cow', \n",
    "            'diningtable','dog','horse','motorbike','person', \n",
    "            'potted plant', 'sheep', 'sofa', 'train', 'monitor','unlabeld'\n",
    "            ]\n",
    "\n",
    "# カラーパレットの作成\n",
    "COLOR_PALETTE = np.array(Image.open(\"./VOCdevkit/VOC2012_sample/SegmentationClass/2007_000170.png\").getpalette()).reshape(-1,3)\n",
    "COLOR_PALETTE = COLOR_PALETTE.tolist()[:len(CLASSES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カラーパレットの可視化\n",
    "fig, axes_list = plt.subplots(len(CLASSES), 1, figsize=(5, 10))\n",
    "for i, color in enumerate(COLOR_PALETTE[:len(CLASSES)]):\n",
    "    color_img = np.full((1, 10, 3), color, dtype=np.uint8)\n",
    "\n",
    "    axes_list[i].imshow(color_img, aspect='auto')\n",
    "    axes_list[i].set_axis_off()\n",
    "    axes_list[i].text(-1, 0, f'{i}: {CLASSES[i]}', va='center', ha='right', fontsize=10)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回はデモなので訓練データ300枚、テストデータ100枚で学習を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データのリストファイル\n",
    "train_list_path = 'VOCdevkit/VOC2012_sample/listfile/train_list_300.txt'\n",
    "val_list_path = 'VOCdevkit/VOC2012_sample/listfile/val_list_100.txt'\n",
    "\n",
    "# データディレクトリ\n",
    "img_dir = 'VOCdevkit/VOC2012_sample/JPEGImages'\n",
    "gt_dir = 'VOCdevkit/VOC2012_sample/SegmentationClass'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# リストファイルの読み込み\n",
    "with open(train_list_path, 'r') as f:\n",
    "    train_list = f.read().splitlines()\n",
    "with open(val_list_path, 'r') as g:\n",
    "    val_list = g.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセット・データローダー\n",
    "- データセット：入力データと正解データをペアで保持するもの（前処理もここで行うことが多い）\n",
    "- データローダー：データセットからサンプルを取得し、実際に取り出すもの"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの作成\n",
    "train_ds = VOCDataset(img_list=train_list, img_dir=img_dir, gt_dir=gt_dir, data_augmentation=False)\n",
    "val_ds = VOCDataset(img_list=val_list, img_dir=img_dir, gt_dir=gt_dir)\n",
    "print(f\"len(train_data): {train_ds.__len__()}\")\n",
    "print(f\"len(test_data): {val_ds.__len__()}\")\n",
    "\n",
    "# データローダーの作成\n",
    "train_dl = DataLoader(train_ds, batch_size=16, num_workers=8, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=16, num_workers=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 関数\n",
    "訓練、テスト、可視化の関数を記述しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練\n",
    "def train(dataloader, model, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    batch_size = dataloader.batch_size\n",
    "    step_total = math.ceil(size/batch_size)\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with tqdm.tqdm(enumerate(dataloader), total=step_total) as pbar:\n",
    "        for batch, item in pbar:\n",
    "            inp, gt = item[0].to(device), item[1].to(device)\n",
    "\n",
    "            # 推論\n",
    "            pred = model(inp)\n",
    "\n",
    "            # 損失誤差を計算\n",
    "            loss = criterion(pred, gt)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # バックプロパゲーション\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # プログレスバーに損失を表示\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_loss = total_loss / step_total\n",
    "        print(f\"Train Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "# テスト\n",
    "def test(dataloader, model, criterion, device):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    batch_size = dataloader.batch_size\n",
    "    step_total = math.ceil(size/batch_size)\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for item in dataloader:\n",
    "            inp, gt = item[0].to(device), item[1].to(device)\n",
    "            pred = model(inp)\n",
    "            loss = criterion(pred, gt)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / step_total\n",
    "        print(f\"Val Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "# 推論結果の可視化\n",
    "def visualize(model, img_id, img_dir, gt_dir, criterion, device='cpu'):\n",
    "    img_path = os.path.join(img_dir, img_id) + \".jpg\"\n",
    "    gt_path = os.path.join(gt_dir, img_id) + \".png\"\n",
    "    class_values = [CLASSES.index(cls.lower()) for cls in CLASSES]\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 入力画像の前処理 (PIL画像 -> Tensor)\n",
    "        inp_tensor = transforms.functional.to_tensor(Image.open(img_path))\n",
    "        inp_resized = transforms.Resize(size=(256, 256))(inp_tensor)\n",
    "        inp_normalized = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(inp_resized)\n",
    "        inp = inp_normalized.unsqueeze(0)\n",
    "\n",
    "        # 正解画像の前処理 (PIL画像 -> NumPy -> Tensor)\n",
    "        gt_np = np.asarray(Image.open(gt_path))\n",
    "        gt_np = np.where(gt_np == 255, 0, gt_np)  # 範囲外の255を0に置換（または別のインデックスに）\n",
    "\n",
    "        gt_tensor = torch.tensor(gt_np, dtype=torch.long)\n",
    "        gt_tensor = gt_tensor.unsqueeze(0)\n",
    "        gt_resized = transforms.Resize(size=(256, 256))(gt_tensor)\n",
    "\n",
    "        # 予測\n",
    "        pred = model(inp)\n",
    "        # loss = criterion(pred, gt_resized) # loss計算\n",
    "        # print(f'Loss: {loss.item()}')\n",
    "\n",
    "        inp_np = (inp_resized.numpy().transpose(1, 2, 0) * 255).astype(np.uint8)  # 表示用スケール\n",
    "        pred_np = torch.argmax(pred, dim=1).cpu().detach().numpy()[0]\n",
    "        pred_np = np.clip(pred_np, 0, len(COLOR_PALETTE) - 1).astype(np.int32)\n",
    "        gt_resized = gt_resized.squeeze(0).detach().numpy().astype(np.int32)\n",
    "\n",
    "        # カラーパレットの適用\n",
    "        img_gt = np.array([[COLOR_PALETTE[gt_resized[i, j]] for j in range(256)] for i in range(256)], dtype=np.uint8)\n",
    "        img_pred = np.array([[COLOR_PALETTE[pred_np[i, j]] for j in range(256)] for i in range(256)], dtype=np.uint8)\n",
    "\n",
    "    # プロット\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    for i, im in enumerate([inp_np, img_gt, img_pred]):\n",
    "        ax = fig.add_subplot(1, 3, i+1)\n",
    "        ax.imshow(im)\n",
    "        ax.set_title([\"Input\", \"Ground Truth\", \"Prediction\"][i])\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. モデル・評価基準の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# デバイスの設定\n",
    "device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "# 保存先の設定\n",
    "save_dir = './unet_demo' # 訓練ログ保存ディレクトリ\n",
    "save_name_prefix = 'unet_demo' # 訓練ログ保存名\n",
    "csv_path = os.path.join(save_dir, f'{save_name_prefix}.csv')\n",
    "\n",
    "# 重みの初期値\n",
    "chkp_path = 'weights/unet_241106_00.pth' # 学習済みの重み\n",
    "continuation = False # 訓練を続きから再開する場合True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの設定\n",
    "今回はUNetを用います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet() # モデル\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 評価基準 （損失関数）\n",
    "- Dice Cross-Entropy Loss（小さいほど高精度）\n",
    "- ダイス損失（Dice Loss）とクロスエントロピー損失を足し合わせた損失関数\n",
    "\n",
    "**ダイス損失（Dice Loss）**  \n",
    "$Dice Loss = 1 - \\frac{2|A \\cap B|}{|A| + |B|}$  \n",
    "<img src='img/dice_loss.png' width=800></img>  \n",
    "引用元：[Dice Loss In Medical Image Segmentation](https://cvinvolution.medium.com/dice-loss-in-medical-image-segmentation-d0e476eb486)\n",
    "\n",
    "**クロスエントロピー損失**  \n",
    "$Cross Entropy Loss = -\\frac{1}{N}\\sum_{i=1}^Np(x)\\log{q(x)}$  \n",
    "<img src='img/cross_entropy.png' width=800></img>  \n",
    "引用元：[自然言語処理：ニューラルネット、クロスエントロピー・ロス関数](https://www.youtube.com/watch?v=MussKg8FfEU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クロスエントロピー損失のクラスごとの重み\n",
    "weight = None # torch.tensor([0.1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5]) \n",
    "\n",
    "# 損失関数のDice Lossの比率\n",
    "dice_weight = 0.5\n",
    "\n",
    "if weight is not None:\n",
    "    weight = weight.to(device)\n",
    "\n",
    "# 損失関数\n",
    "criterion =  DiceCrossEntropyLoss(weight=weight, dice_weight=dice_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最適化手法\n",
    "損失関数を最小化するために、モデルのパラメータを更新するアルゴリズム  \n",
    "例）確率的勾配降下法、モーメンタム、AdaGradなど\n",
    "\n",
    "今回は**Adam（Adaptive Moment Estimation）**というものを用いる。\n",
    ">**学習率**  \n",
    ">パラメータ（モデルの重みなど）をどれだけの大きさで更新するかを決める重要なハイパーパラメータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01 # 学習率\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr) # 最適化手法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### エポック\n",
    "訓練データを何回使ったかを表す数⇒何回訓練を行うか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epoch = 0 # ここは変えない\n",
    "epochs = 10 # エポック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存ディレクトリの作成\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "# 重みの読み込み\n",
    "if chkp_path is not None:\n",
    "    checkpoint = torch.load(chkp_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "    if continuation == True:\n",
    "        initial_epoch = checkpoint[\"epoch\"] + 1\n",
    "\n",
    "for epoch in range(initial_epoch, epochs):\n",
    "    print(f\"Epoch {epoch}\\n-------------------------------\")\n",
    "    train_loss = train(train_dl, model, optimizer, criterion, device)\n",
    "    val_loss = test(val_dl, model, criterion, device)\n",
    "    # 訓練ログ記入\n",
    "    if not os.path.exists(csv_path):\n",
    "        with open(csv_path, 'w') as f:\n",
    "            f.write('epoch,train_loss,val_loss\\n')\n",
    "    with open(csv_path, 'a') as f:\n",
    "        f.write(f'{epoch},{train_loss},{val_loss}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 結果の可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 損失のプロット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = pd.read_csv(csv_path)\n",
    "plt.plot(log_df['epoch'], log_df['train_loss'], label='trian_data')\n",
    "plt.plot(log_df['epoch'], log_df['val_loss'], label='test_data')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "# plt.ylim(0,2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推論結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>train data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = '2007_000032'\n",
    "visualize(model, img_id, img_dir, gt_dir, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = '2007_000648'\n",
    "visualize(model, img_id, img_dir, gt_dir, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = '2007_001225'\n",
    "visualize(model, img_id, img_dir, gt_dir, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = '2007_002488'\n",
    "visualize(model, img_id, img_dir, gt_dir, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>test data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = '2007_000033'\n",
    "visualize(model, img_id, img_dir, gt_dir, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = '2007_001763'\n",
    "visualize(model, img_id, img_dir, gt_dir, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = '2007_003367'\n",
    "visualize(model, img_id, img_dir, gt_dir, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
